{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A\n",
    "\n",
    "## Standard Section 8: Multiclass Model and Midterm Review\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Section Leaders: Albert Wu, Nathaniel Burbank<br/>**\n",
    "**Instructors: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>**Download this notebook from the CS109 repo or here:**</center>\n",
    "<center>**http://bit.ly/Sec_8_109a**</center>\n",
    "\n",
    "This section can be split into two major parts. The first covers multiclass model fitting and is designed to help you get started on HW 7. The second part goes over midterm 2 from 2016 and is designed to help you prepare for the upcoming midterm. \n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "    1. Use the iris dataset, which we used in section 3, to cover multiclass models and fitting them.\n",
    "    2. Dive into midterm 2 from last year and go through a thorough review of the solutions that were posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Fitting Multiclass Models to the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first part, part 0, is designed to help you get started on the first part of the homework. The dataset we use is similar to that of section 3, where we had flower type as our predictor (0,1,2). Here we will focus on sepal length and petal width as our predictors. We will fit and compare the training and test accuracies of the following classification methods:\n",
    "\n",
    "- Multiclass Logistic Regression (Multinomial and one-vs-rest (OvR))\n",
    "- Multiclass Logistic Regression cubic terms\n",
    "- Linear Discriminant Analysis\n",
    "- Quadratic Discriminant Analysis\n",
    "- k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slength</th>\n",
       "      <th>pwidth</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   slength  pwidth  target\n",
       "0      5.1     0.2       0\n",
       "1      4.9     0.2       0\n",
       "2      4.7     0.2       0\n",
       "4      5.0     0.2       0\n",
       "5      5.4     0.4       0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/cs109/a-2017/master/Sections/Standard/s8_data/section_8_data.csv')\n",
    "msk = np.random.rand(len(df)) < 0.5\n",
    "data_train_2 = df[msk]\n",
    "data_test_2 = df[~msk]\n",
    "data_train_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `target` contains our three types of flowers (0,1,2), while `slength` and `pwidth` are the sepal legnth and petal width for each specific flower observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x119592278>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118bb7f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training points\n",
    "plt.scatter(data_train_2.iloc[:, 0], data_train_2.iloc[:, 1], c=data_train_2.iloc[:,2].values, cmap=plt.cm.bwr,\n",
    "            edgecolor='k')\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Petal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that for our case, a linear classifier will not be able to perfectly separate the points. But it does appear that a linear classifier should seem to do relatively well here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use multinomial logistic regression and one-vs-rest (OvR) logistic regression methods for fitting a multiclass classifier. In OvR, a separate binary classifier is fit for each class, whereas in multinomial logistic regression a single classifier is fit for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = data_train_2[['slength', 'pwidth']]\n",
    "y_train_2 = data_train_2['target']\n",
    "X_test_2 = data_test_2[['slength', 'pwidth']]\n",
    "y_test_2 = data_test_2['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will fit both a OvR logistic regression and Multinomial logistic regression model. Further, we will look at the training and testing accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train OvR: 0.923076923077\n",
      "Test OvR: 0.861111111111\n",
      "Train Multinomial: 0.961538461538\n",
      "Test Multinomial: 0.958333333333\n"
     ]
    }
   ],
   "source": [
    "logregcv = LogisticRegressionCV(multi_class='ovr')\n",
    "logregcv.fit(X_train_2, y_train_2)\n",
    "print(\"Train OvR:\", logregcv.score(X_train_2, y_train_2))\n",
    "print(\"Test OvR:\", logregcv.score(X_test_2, y_test_2))\n",
    "logregcv_2 = LogisticRegressionCV(multi_class='multinomial')\n",
    "logregcv_2.fit(X_train_2, y_train_2)\n",
    "print(\"Train Multinomial:\", logregcv_2.score(X_train_2, y_train_2))\n",
    "print(\"Test Multinomial:\",logregcv_2.score(X_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we show how to fit a Multiclass Logistic Regression model with cubic terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(3)\n",
    "X_train_poly_cubic = poly.fit_transform(X_train_2)\n",
    "X_test_poly_cubic = poly.fit_transform(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train LR Cubic Features OvR: 0.961538461538\n",
      "Test LR Cubic Features OvR: 0.930555555556\n",
      "Train LR Cubic Features Multinomial: 0.961538461538\n",
      "Test LR Cubic Features Multinomial: 0.958333333333\n"
     ]
    }
   ],
   "source": [
    "logregcv_cubic = LogisticRegressionCV()\n",
    "logregcv_cubic.fit(X_train_poly_cubic, y_train_2)\n",
    "print(\"Train LR Cubic Features OvR:\", logregcv_cubic.score(X_train_poly_cubic, y_train_2))\n",
    "print(\"Test LR Cubic Features OvR:\", logregcv_cubic.score(X_test_poly_cubic, y_test_2))\n",
    "logregcv_2_poly_cubic = LogisticRegressionCV(multi_class='multinomial')\n",
    "logregcv_2_poly_cubic.fit(X_train_poly_cubic, y_train_2)\n",
    "print(\"Train LR Cubic Features Multinomial:\", logregcv_2_poly_cubic.score(X_train_poly_cubic, y_train_2))\n",
    "print(\"Test LR Cubic Features Multinomial:\",logregcv_2_poly_cubic.score(X_test_poly_cubic, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compare our results to that given by LDA and QDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train LDA: 0.961538461538\n",
      "Test LDA: 0.958333333333\n",
      "Train QDA: 0.961538461538\n",
      "Test QDA: 0.958333333333\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "lda.fit(X_train_2, y_train_2)\n",
    "print(\"Train LDA:\", lda.score(X_train_2, y_train_2))\n",
    "print(\"Test LDA:\", lda.score(X_test_2, y_test_2))\n",
    "qda = QDA()\n",
    "qda.fit(X_train_2, y_train_2)\n",
    "print(\"Train QDA:\", qda.score(X_train_2, y_train_2))\n",
    "print(\"Test QDA:\",qda.score(X_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we cover the fitting of k-Nearest Neighbors for $k = 1,2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train KNN 1: 0.974358974359\n",
      "Test KNN 1: 0.930555555556\n",
      "Train KNN 2: 0.961538461538\n",
      "Test KNN 2: 0.930555555556\n"
     ]
    }
   ],
   "source": [
    "knn_1 = KNN(n_neighbors=1)\n",
    "knn_1.fit(X_train_2, y_train_2)\n",
    "print(\"Train KNN 1:\", knn_1.score(X_train_2, y_train_2))\n",
    "print(\"Test KNN 1:\", knn_1.score(X_test_2, y_test_2))\n",
    "knn_2 = KNN(n_neighbors=2)\n",
    "knn_2.fit(X_train_2, y_train_2)\n",
    "print(\"Train KNN 2:\", knn_2.score(X_train_2, y_train_2))\n",
    "print(\"Test KNN 2:\", knn_2.score(X_test_2, y_test_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now move on to the Midterm 2 solutions from last year:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Diagnosing the Simian Flu 2016\n",
    "\n",
    "You are given the early data for an outbreak of a dangerous virus originating from a group of primates being kept in a Massechussetts biomedical research lab, this virus is dubbed the \"Simian Flu\".\n",
    "\n",
    "You have the medical records of $n$ number of patients in `'flu_train.csv`. There are two general types of patients in the data, flu patients and healthy (this is recorded in the column labeled `flu`, a 0 indicates the absences of the virus and a 1 indicates presence). Furthermore, scientists have found that there are two strains of the virus, each requiring a different type of treatment (this is recorded in the column labeled `flutype`, a 1 indicates the absences of the virus, a 2 indicates presence of strain 1 and a 3 indicates the presence of strain 2).\n",
    "\n",
    "**Your task:** build a model to predict if a given patient has the flu. Your goal is to catch as many flu patients as possible without misdiagnosing too many healthy patients.\n",
    "\n",
    "**The deliverable:** a function called `flu_predict` which satisfies:\n",
    "\n",
    "- input: `x_test`, a set of medical predictors for a group of patients\n",
    "- output: `y_pred`, a set of labels, one for each patient; 0 for healthy and 1 for infected with the flu virus\n",
    "\n",
    "The MA state government will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "We provide you with some benchmarks for comparison.\n",
    "\n",
    "**Baseline Model:** \n",
    "- ~50% expected accuracy on healthy patients in observed data\n",
    "- ~50% expected accuracy on flu patients in observed data\n",
    "- ~50% expected accuracy on healthy patients in future data \n",
    "- ~50% expected accuracy on flu patients in future data\n",
    "- time to build: 5 min\n",
    "\n",
    "**Reasonable Model:** \n",
    "- ~69% expected accuracy on healthy patients in observed data\n",
    "- ~55% expected accuracy on flu patients, in observed data\n",
    "- ~69% expected accuracy on healthy patients in future data\n",
    "- ~60% expected accuracy on flu patients, in future data\n",
    "- time to build: 20 min\n",
    "\n",
    "**Grading:**\n",
    "Your grade will be based on:\n",
    "1. your model's ability to out-perform our benchmarks\n",
    "2. your ability to carefully and thoroughly follow the data science pipeline (see lecture slides for definition)\n",
    "3. the extend to which all choices are reasonable and defensible by methods you have learned in this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions:**\n",
    "\n",
    "## Step 1: Read the data, clean and explore the data\n",
    "\n",
    "There are a large number of missing values in the data. Nearly all predictors have some degree of missingness. Not all missingness are alike: as Mike points out, NaN in the `'pregnancy'` column is meaningful and informative, as patients with NaN's in the pregnancy column are males, where as NaN's in other predictors may appear randomly. \n",
    "\n",
    "\n",
    "**What we do:** We make no attempt to interpret the predictors and we make no attempt to model the missing values in the data in any meaningful way. We replace all missing values with 0.\n",
    "\n",
    "However, it would be more complete to look at the data and allow the data to inform your decision on how to address missingness. For columns where NaN values are informative, you might want to treat NaN as a distinct value; You might want to drop predictors with too many missing values and impute the ones with few missing values using KNN or a parametric model. There are many acceptable strategies here, as long as the appropriateness of the method in the context of the task and the data is discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeDecade</th>\n",
       "      <th>AgeMonths</th>\n",
       "      <th>Race1</th>\n",
       "      <th>Race3</th>\n",
       "      <th>Education</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>HHIncome</th>\n",
       "      <th>...</th>\n",
       "      <th>HardDrugs</th>\n",
       "      <th>SexEver</th>\n",
       "      <th>SexAge</th>\n",
       "      <th>SexNumPartnLife</th>\n",
       "      <th>SexNumPartYear</th>\n",
       "      <th>SameSex</th>\n",
       "      <th>SexOrientation</th>\n",
       "      <th>PregnantNow</th>\n",
       "      <th>flu</th>\n",
       "      <th>flutype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51624</td>\n",
       "      <td>male</td>\n",
       "      <td>34</td>\n",
       "      <td>30-39</td>\n",
       "      <td>409.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High School</td>\n",
       "      <td>Married</td>\n",
       "      <td>25000-34999</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>female</td>\n",
       "      <td>49</td>\n",
       "      <td>40-49</td>\n",
       "      <td>596.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some College</td>\n",
       "      <td>LivePartner</td>\n",
       "      <td>35000-44999</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51638</td>\n",
       "      <td>male</td>\n",
       "      <td>9</td>\n",
       "      <td>0-9</td>\n",
       "      <td>115.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75000-99999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51646</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>0-9</td>\n",
       "      <td>101.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55000-64999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51647</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>40-49</td>\n",
       "      <td>541.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>College Grad</td>\n",
       "      <td>Married</td>\n",
       "      <td>75000-99999</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bisexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Gender  Age AgeDecade  AgeMonths  Race1 Race3     Education  \\\n",
       "0  51624    male   34     30-39      409.0  White   NaN   High School   \n",
       "1  51630  female   49     40-49      596.0  White   NaN  Some College   \n",
       "2  51638    male    9       0-9      115.0  White   NaN           NaN   \n",
       "3  51646    male    8       0-9      101.0  White   NaN           NaN   \n",
       "4  51647  female   45     40-49      541.0  White   NaN  College Grad   \n",
       "\n",
       "  MaritalStatus     HHIncome   ...     HardDrugs  SexEver  SexAge  \\\n",
       "0       Married  25000-34999   ...           Yes      Yes    16.0   \n",
       "1   LivePartner  35000-44999   ...           Yes      Yes    12.0   \n",
       "2           NaN  75000-99999   ...           NaN      NaN     NaN   \n",
       "3           NaN  55000-64999   ...           NaN      NaN     NaN   \n",
       "4       Married  75000-99999   ...            No      Yes    13.0   \n",
       "\n",
       "  SexNumPartnLife SexNumPartYear  SameSex  SexOrientation  PregnantNow  flu  \\\n",
       "0             8.0            1.0       No    Heterosexual          NaN    0   \n",
       "1            10.0            1.0      Yes    Heterosexual          NaN    0   \n",
       "2             NaN            NaN      NaN             NaN          NaN    0   \n",
       "3             NaN            NaN      NaN             NaN          NaN    0   \n",
       "4            20.0            0.0      Yes        Bisexual          NaN    0   \n",
       "\n",
       "   flutype  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/cs109/a-2017/master/Midterms/2016%20Midterm%202/data/flu_train.csv')\n",
    "\n",
    "df = df[~np.isnan(df['flu'])]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeDecade</th>\n",
       "      <th>AgeMonths</th>\n",
       "      <th>Race1</th>\n",
       "      <th>Race3</th>\n",
       "      <th>Education</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>HHIncome</th>\n",
       "      <th>...</th>\n",
       "      <th>HardDrugs</th>\n",
       "      <th>SexEver</th>\n",
       "      <th>SexAge</th>\n",
       "      <th>SexNumPartnLife</th>\n",
       "      <th>SexNumPartYear</th>\n",
       "      <th>SameSex</th>\n",
       "      <th>SexOrientation</th>\n",
       "      <th>PregnantNow</th>\n",
       "      <th>flu</th>\n",
       "      <th>flutype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51624</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>409.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>596.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51638</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51646</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>101.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51647</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>541.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Gender  Age  AgeDecade  AgeMonths  Race1  Race3  Education  \\\n",
       "0  51624       1   34          4      409.0      4      0          4   \n",
       "1  51630       0   49          5      596.0      4      0          5   \n",
       "2  51638       1    9          1      115.0      4      0          0   \n",
       "3  51646       1    8          1      101.0      4      0          0   \n",
       "4  51647       0   45          5      541.0      4      0          3   \n",
       "\n",
       "   MaritalStatus  HHIncome   ...     HardDrugs  SexEver  SexAge  \\\n",
       "0              3         6   ...             2        2    16.0   \n",
       "1              2         7   ...             2        2    12.0   \n",
       "2              0        11   ...             0        0     0.0   \n",
       "3              0         9   ...             0        0     0.0   \n",
       "4              3        11   ...             1        2    13.0   \n",
       "\n",
       "   SexNumPartnLife  SexNumPartYear  SameSex  SexOrientation  PregnantNow  flu  \\\n",
       "0              8.0             1.0        1               2            0    0   \n",
       "1             10.0             1.0        2               2            0    0   \n",
       "2              0.0             0.0        0               0            0    0   \n",
       "3              0.0             0.0        0               0            0    0   \n",
       "4             20.0             0.0        2               1            0    0   \n",
       "\n",
       "   flutype  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean and encode\n",
    "\n",
    "encode = preprocessing.LabelEncoder()\n",
    "\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == np.object:\n",
    "        df[column] = df[column].fillna('')\n",
    "        df.loc[:, column] = encode.fit_transform(df[column])\n",
    "        \n",
    "df = df.fillna(0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeDecade</th>\n",
       "      <th>AgeMonths</th>\n",
       "      <th>Race1</th>\n",
       "      <th>Race3</th>\n",
       "      <th>Education</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>HHIncome</th>\n",
       "      <th>...</th>\n",
       "      <th>HardDrugs</th>\n",
       "      <th>SexEver</th>\n",
       "      <th>SexAge</th>\n",
       "      <th>SexNumPartnLife</th>\n",
       "      <th>SexNumPartYear</th>\n",
       "      <th>SameSex</th>\n",
       "      <th>SexOrientation</th>\n",
       "      <th>PregnantNow</th>\n",
       "      <th>flu</th>\n",
       "      <th>flutype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51624</td>\n",
       "      <td>male</td>\n",
       "      <td>34</td>\n",
       "      <td>30-39</td>\n",
       "      <td>409.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High School</td>\n",
       "      <td>Married</td>\n",
       "      <td>25000-34999</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>female</td>\n",
       "      <td>49</td>\n",
       "      <td>40-49</td>\n",
       "      <td>596.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Some College</td>\n",
       "      <td>LivePartner</td>\n",
       "      <td>35000-44999</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Heterosexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51638</td>\n",
       "      <td>male</td>\n",
       "      <td>9</td>\n",
       "      <td>0-9</td>\n",
       "      <td>115.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75000-99999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51646</td>\n",
       "      <td>male</td>\n",
       "      <td>8</td>\n",
       "      <td>0-9</td>\n",
       "      <td>101.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55000-64999</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51647</td>\n",
       "      <td>female</td>\n",
       "      <td>45</td>\n",
       "      <td>40-49</td>\n",
       "      <td>541.0</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>College Grad</td>\n",
       "      <td>Married</td>\n",
       "      <td>75000-99999</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bisexual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Gender  Age AgeDecade  AgeMonths  Race1 Race3     Education  \\\n",
       "0  51624    male   34     30-39      409.0  White   NaN   High School   \n",
       "1  51630  female   49     40-49      596.0  White   NaN  Some College   \n",
       "2  51638    male    9       0-9      115.0  White   NaN           NaN   \n",
       "3  51646    male    8       0-9      101.0  White   NaN           NaN   \n",
       "4  51647  female   45     40-49      541.0  White   NaN  College Grad   \n",
       "\n",
       "  MaritalStatus     HHIncome   ...     HardDrugs  SexEver  SexAge  \\\n",
       "0       Married  25000-34999   ...           Yes      Yes    16.0   \n",
       "1   LivePartner  35000-44999   ...           Yes      Yes    12.0   \n",
       "2           NaN  75000-99999   ...           NaN      NaN     NaN   \n",
       "3           NaN  55000-64999   ...           NaN      NaN     NaN   \n",
       "4       Married  75000-99999   ...            No      Yes    13.0   \n",
       "\n",
       "  SexNumPartnLife SexNumPartYear  SameSex  SexOrientation  PregnantNow  flu  \\\n",
       "0             8.0            1.0       No    Heterosexual          NaN    0   \n",
       "1            10.0            1.0      Yes    Heterosexual          NaN    0   \n",
       "2             NaN            NaN      NaN             NaN          NaN    0   \n",
       "3             NaN            NaN      NaN             NaN          NaN    0   \n",
       "4            20.0            0.0      Yes        Bisexual          NaN    0   \n",
       "\n",
       "   flutype  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/cs109/a-2017/master/Midterms/2016%20Midterm%202/data/flu_train.csv')\n",
    "\n",
    "df_test = df_test[~np.isnan(df_test['flu'])]\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>AgeDecade</th>\n",
       "      <th>AgeMonths</th>\n",
       "      <th>Race1</th>\n",
       "      <th>Race3</th>\n",
       "      <th>Education</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>HHIncome</th>\n",
       "      <th>...</th>\n",
       "      <th>HardDrugs</th>\n",
       "      <th>SexEver</th>\n",
       "      <th>SexAge</th>\n",
       "      <th>SexNumPartnLife</th>\n",
       "      <th>SexNumPartYear</th>\n",
       "      <th>SameSex</th>\n",
       "      <th>SexOrientation</th>\n",
       "      <th>PregnantNow</th>\n",
       "      <th>flu</th>\n",
       "      <th>flutype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51624</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>409.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>596.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51638</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51646</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>101.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51647</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>541.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  Gender  Age  AgeDecade  AgeMonths  Race1  Race3  Education  \\\n",
       "0  51624       1   34          4      409.0      4      0          4   \n",
       "1  51630       0   49          5      596.0      4      0          5   \n",
       "2  51638       1    9          1      115.0      4      0          0   \n",
       "3  51646       1    8          1      101.0      4      0          0   \n",
       "4  51647       0   45          5      541.0      4      0          3   \n",
       "\n",
       "   MaritalStatus  HHIncome   ...     HardDrugs  SexEver  SexAge  \\\n",
       "0              3         6   ...             2        2    16.0   \n",
       "1              2         7   ...             2        2    12.0   \n",
       "2              0        11   ...             0        0     0.0   \n",
       "3              0         9   ...             0        0     0.0   \n",
       "4              3        11   ...             1        2    13.0   \n",
       "\n",
       "   SexNumPartnLife  SexNumPartYear  SameSex  SexOrientation  PregnantNow  flu  \\\n",
       "0              8.0             1.0        1               2            0    0   \n",
       "1             10.0             1.0        2               2            0    0   \n",
       "2              0.0             0.0        0               0            0    0   \n",
       "3              0.0             0.0        0               0            0    0   \n",
       "4             20.0             0.0        2               1            0    0   \n",
       "\n",
       "   flutype  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean and encode\n",
    "\n",
    "encode = preprocessing.LabelEncoder()\n",
    "\n",
    "for column in df_test.columns:\n",
    "    if df_test[column].dtype == np.object:\n",
    "        df_test[column] = df_test[column].fillna('')\n",
    "        df_test.loc[:, column] = encode.fit_transform(df_test[column])\n",
    "        \n",
    "df_test = df_test.fillna(0)\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x train shape: (5246, 74)\n",
      "x test shape: (5246, 74)\n",
      "train class 0: 4936, train class 1: 310\n",
      "train class 0: 4936, train class 1: 310\n"
     ]
    }
   ],
   "source": [
    "#What's up in each set\n",
    "\n",
    "x = df.values[:, :-2]\n",
    "y = df.values[:, -2]\n",
    "\n",
    "x_test = df_test.values[:, :-2]\n",
    "y_test = df_test.values[:, -2]\n",
    "\n",
    "print('x train shape:', x.shape)\n",
    "print('x test shape:', x_test.shape)\n",
    "print('train class 0: {}, train class 1: {}'.format(len(y[y==0]), len(y[y==1])))\n",
    "print('train class 0: {}, train class 1: {}'.format(len(y_test[y_test==0]), len(y_test[y_test==1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Choice\n",
    "\n",
    "The first task is to decide which, of the large number of classifiers we have learned during this semester, would best suit our task and our data.\n",
    "\n",
    "It would be possible to do brute force model comparison here - i.e. tune all models and compare which does best with respect to various benchmarks. However, it is also reasonable to do a first round of model comparison by running models (with out of the box parameter settings) on the training data and eliminating models which performed very poorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_score(model, x_test, y_test):\n",
    "    overall = 0\n",
    "    class_0 = 0\n",
    "    class_1 = 0\n",
    "    for i in range(100):\n",
    "        np.random.seed(i)\n",
    "        sample = np.random.choice(len(x_test), len(x_test))\n",
    "        x_sub_test = x_test[sample]\n",
    "        y_sub_test = y_test[sample]\n",
    "        \n",
    "        overall += model.score(x_sub_test, y_sub_test)\n",
    "        class_0 += model.score(x_sub_test[y_sub_test==0], y_sub_test[y_sub_test==0])\n",
    "        class_1 += model.score(x_sub_test[y_sub_test==1], y_sub_test[y_sub_test==1])\n",
    "\n",
    "    return pd.Series([overall / 100., \n",
    "                      class_0 / 100.,\n",
    "                      class_1 / 100.],\n",
    "                      index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])\n",
    "\n",
    "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n",
    "                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n",
    "                                                 model.score(x_test[y_test==1], y_test[y_test==1])], \n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "unweighted log\n",
      "weighted log\n",
      "lda\n",
      "qda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:695: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree\n",
      "rf\n",
      "svc\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn = KNN(n_neighbors=2)\n",
    "knn.fit(x, y)\n",
    "\n",
    "knn_scores = score(knn, x, y)\n",
    "print('knn')\n",
    "\n",
    "#Unweighted logistic regression\n",
    "unweighted_logistic = LogisticRegression(C=1000)\n",
    "unweighted_logistic.fit(x, y)\n",
    "\n",
    "unweighted_log_scores = score(unweighted_logistic, x, y)\n",
    "print('unweighted log')\n",
    "\n",
    "\n",
    "#Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(C=1000, class_weight='balanced')\n",
    "weighted_logistic.fit(x, y)\n",
    "\n",
    "weighted_log_scores = score(weighted_logistic, x, y)\n",
    "print('weighted log')\n",
    "\n",
    "\n",
    "#LDA\n",
    "lda = LDA()\n",
    "lda.fit(x, y)\n",
    "\n",
    "lda_scores = score(lda, x, y)\n",
    "print('lda')\n",
    "\n",
    "#QDA\n",
    "qda = QDA()\n",
    "qda.fit(x, y)\n",
    "\n",
    "qda_scores = score(qda, x, y)\n",
    "print('qda')\n",
    "\n",
    "#Decision Tree\n",
    "tree = DecisionTree(max_depth=50, class_weight='balanced', criterion='entropy')\n",
    "tree.fit(x, y)\n",
    "\n",
    "tree_scores = score(tree, x, y)\n",
    "print('tree')\n",
    "\n",
    "\n",
    "#Random Forest\n",
    "rf = RandomForest(class_weight='balanced')\n",
    "rf.fit(x, y)\n",
    "\n",
    "rf_scores = score(rf, x, y)\n",
    "\n",
    "print('rf')\n",
    "\n",
    "#SVC\n",
    "svc = SVC(C=100, class_weight='balanced')\n",
    "svc.fit(x, y)\n",
    "\n",
    "svc_scores = score(svc, x, y)\n",
    "\n",
    "print('svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knn</th>\n",
       "      <th>lda</th>\n",
       "      <th>qda</th>\n",
       "      <th>rf</th>\n",
       "      <th>svc</th>\n",
       "      <th>tree</th>\n",
       "      <th>unweighted logistic</th>\n",
       "      <th>weighted logistic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.945864</td>\n",
       "      <td>0.937476</td>\n",
       "      <td>0.858559</td>\n",
       "      <td>0.984369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.941289</td>\n",
       "      <td>0.704346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985616</td>\n",
       "      <td>0.869327</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.705632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>0.083871</td>\n",
       "      <td>0.170968</td>\n",
       "      <td>0.687097</td>\n",
       "      <td>0.735484</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009677</td>\n",
       "      <td>0.683871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          knn       lda       qda        rf  svc  tree  \\\n",
       "overall accuracy     0.945864  0.937476  0.858559  0.984369  1.0   1.0   \n",
       "accuracy on class 0  1.000000  0.985616  0.869327  1.000000  1.0   1.0   \n",
       "accuracy on class 1  0.083871  0.170968  0.687097  0.735484  1.0   1.0   \n",
       "\n",
       "                     unweighted logistic  weighted logistic  \n",
       "overall accuracy                0.941289           0.704346  \n",
       "accuracy on class 0             0.999797           0.705632  \n",
       "accuracy on class 1             0.009677           0.683871  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Score Dataframe\n",
    "score_df = pd.DataFrame({'knn': knn_scores, \n",
    "                         'unweighted logistic': unweighted_log_scores,\n",
    "                         'weighted logistic': weighted_log_scores,\n",
    "                         'lda': lda_scores,\n",
    "                         'qda': qda_scores,\n",
    "                         'tree': tree_scores,\n",
    "                         'rf': rf_scores, \n",
    "                         'svc': svc_scores})\n",
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we can rule out KNN, LDA and unweighted logistic. \n",
    "\n",
    "**What we do:** We are going to pick weighted logistic regression and just tune the regularization parameter to beat the test benchmarks. Can you figure out why we chose this course of action? Hint: Instead of looking at overall accuracy, look at accuracy on class 1. \n",
    "\n",
    "**What's probably good to do:** QDA, random forest, tree, SVC and weighted logistic are beating our train benchmarks as is. We will tune them to beat the test benchmarks by picking the model and parameter set with the highest CV accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.001\n",
      "C: 0.01\n",
      "C: 0.1\n",
      "C: 1.0\n",
      "C: 10.0\n",
      "C: 100.0\n",
      "C: 1000.0\n"
     ]
    }
   ],
   "source": [
    "Cs = 10.**np.arange(-3, 4, 1)\n",
    "scores = []\n",
    "for C in Cs:\n",
    "    print('C:', C)\n",
    "    weighted_log_scores = np.array([0., 0., 0.])\n",
    "    kf = KFold(len(x), n_folds=10, shuffle=True, random_state=10)\n",
    "    for train_index, test_index in kf:\n",
    "        x_validate_train, x_validate_test = x[train_index], x[test_index]\n",
    "        y_validate_train, y_validate_test = y[train_index], y[test_index]\n",
    "\n",
    "        weighted_logistic = LogisticRegression(C=C, class_weight='balanced')\n",
    "        weighted_logistic.fit(x_validate_train, y_validate_train)\n",
    "\n",
    "        weighted_log_scores += score(weighted_logistic, x_validate_test, y_validate_test).values\n",
    "\n",
    "    scores.append(weighted_log_scores / 10.)\n",
    "\n",
    "scores = pd.DataFrame(np.array(scores).T, columns=[str(C) for C in Cs], index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What C means here:** The `C` parameter in the `LogisticRegression` method is the inverse of the regularization parameter $\\lambda$. In other words, `C` $= \\frac{1}{\\lambda}$. As C gets larger, $\\lambda$ is getting smaller. Therefore, when we set `C` to be very very large in our earlier homework assignments to make sure there was **no** regularization, this was the same as making $\\lambda$ go to zero. When $\\lambda$ is zero, there is no regularization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.1</th>\n",
       "      <th>1.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>100.0</th>\n",
       "      <th>1000.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.715398</td>\n",
       "      <td>0.702053</td>\n",
       "      <td>0.694617</td>\n",
       "      <td>0.694044</td>\n",
       "      <td>0.695762</td>\n",
       "      <td>0.695380</td>\n",
       "      <td>0.692521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 0</th>\n",
       "      <td>0.724678</td>\n",
       "      <td>0.710710</td>\n",
       "      <td>0.702634</td>\n",
       "      <td>0.701603</td>\n",
       "      <td>0.703431</td>\n",
       "      <td>0.702819</td>\n",
       "      <td>0.700215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>0.571205</td>\n",
       "      <td>0.569065</td>\n",
       "      <td>0.568667</td>\n",
       "      <td>0.574649</td>\n",
       "      <td>0.574465</td>\n",
       "      <td>0.576891</td>\n",
       "      <td>0.569549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0.001      0.01       0.1       1.0      10.0  \\\n",
       "overall accuracy     0.715398  0.702053  0.694617  0.694044  0.695762   \n",
       "accuracy on class 0  0.724678  0.710710  0.702634  0.701603  0.703431   \n",
       "accuracy on class 1  0.571205  0.569065  0.568667  0.574649  0.574465   \n",
       "\n",
       "                        100.0    1000.0  \n",
       "overall accuracy     0.695380  0.692521  \n",
       "accuracy on class 0  0.702819  0.700215  \n",
       "accuracy on class 1  0.576891  0.569549  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To beat the future benchmark, we'll select the parameter which yields the highest accuracy on class 1 (while still beating the benchmark on class 0).\n",
    "\n",
    "Now let's test our model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall accuracy       0.703584\n",
       "accuracy on class 0    0.705024\n",
       "accuracy on class 1    0.680645\n",
       "dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(C=100, class_weight='balanced')\n",
    "weighted_logistic.fit(x, y)\n",
    "weighted_log_scores = score(weighted_logistic, x_test, y_test)\n",
    "weighted_log_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we beat all the benchmarks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Diagnosing Strains of the Simian Flu\n",
    "\n",
    "From a public health perspective, we want to balance the cost of vaccinations, early interventions and the cost of treating flu complications of unvaccinated people. \n",
    "\n",
    "There are two different strains of the flu: strain 1 has a cheaper early intervention as well as a cheaper treatment for flu complications, but patients with strain 1 has a higher rate of developing complications if treated with the wrong intervention. Strain 2 has a more expensive early intervention as well as a more costly treatment for flu complications, but patients with strain 2 has a lower rate of developing complications if treated with the wrong intervention. With no intervention, flu patients develop complications at the same rate regardless of the strain. \n",
    "\n",
    "**Your task:** build a model to predict if a given patient has the flu and identify the flu strain. The state government of MA will use your model to inform public health policies: we will vaccinate people you've identified as healthy and apply corresponding interventions to patients with different strains of the flu. We have provided you with a function to compute the total expected cost of this policy decision that takes into account the cost of the vaccine, the interventions and the cost of the treatments for flu complications resulting from misdiagnosing patients. Your goal is to make sure your model produces a public health policy with the lowest associated expected cost.\n",
    "\n",
    "**The deliverable:** a function called `flu_predict` which satisfies:\n",
    "\n",
    "- input: `x_test`, a set of medical predictors for a group of patients\n",
    "- output: `y_pred`, a set of labels, one for each patient; 1 for healthy, 2 for infected with strain 1, and 3 for infected with strain 2.\n",
    "\n",
    "The MA state government will use your model to diagnose sets of future patients (held by us). You can expect that there will be an increase in the number of flu patients in any groups of patients in the future.\n",
    "\n",
    "We provide you with some benchmarks for comparison.\n",
    "\n",
    "**Three Baseline Models:** \n",
    "- expected cost on observed data: \\$6,818,206.0, \\$7,035,735.0, \\$8,297,197.5\n",
    "- time to build: 1 min\n",
    "\n",
    "**Reasonable Model:** \n",
    "- expected cost on observed data: $6,300,000\n",
    "- time to build: 20 min\n",
    "\n",
    "**Grading:**\n",
    "Your grade will be based on:\n",
    "1. your model's ability to out-perform our benchmarks\n",
    "2. your ability to carefully and thoroughly follow the data science pipeline (see lecture slides for definition)\n",
    "3. the extend to which all choices are reasonable and defensible by methods you have learned in this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  cost\n",
    "# A function that computes the expected cost of the public healthy policy based on the \n",
    "# classifications generated by your model\n",
    "# Input: \n",
    "#      y_true (true class labels: 0, 1, 2)\n",
    "#      y_pred (predicted class labels: 0, 1, 2)\n",
    "# Returns: \n",
    "#      total_cost (expected total cost)\n",
    "\n",
    "def cost(y_true, y_pred):\n",
    "    cost_of_treatment_1 = 29500\n",
    "    cost_of_treatment_2 = 45000\n",
    "    cost_of_intervention_1 = 4150\n",
    "    cost_of_intervention_2 = 4250\n",
    "    cost_of_vaccine = 15\n",
    "    \n",
    "    prob_complications_untreated = 0.65\n",
    "    prob_complications_1 = 0.30\n",
    "    prob_complications_2 = 0.15\n",
    "    \n",
    "    trials = 1000    \n",
    "    \n",
    "    intervention_cost = cost_of_intervention_1 * len(y_pred[y_pred==1]) + cost_of_intervention_2 * len(y_pred[y_pred==2])\n",
    "\n",
    "    vaccine_cost = cost_of_vaccine * len(y_pred[y_pred==0])\n",
    "    \n",
    "    false_neg_1 = ((y_true == 1) & (y_pred == 2)).sum()\n",
    "    false_neg_2 = ((y_true == 2) & (y_pred == 1)).sum()\n",
    "    \n",
    "    untreated_1 = ((y_true == 1) & (y_pred == 0)).sum()    \n",
    "    untreated_2 = ((y_true == 2) & (y_pred == 0)).sum()\n",
    "    \n",
    "    false_neg_1_cost = np.random.binomial(1, prob_complications_1, (false_neg_1, trials)) * cost_of_treatment_1\n",
    "    false_neg_2_cost = np.random.binomial(1, prob_complications_2, (false_neg_2, trials)) * cost_of_treatment_2\n",
    "    untreated_1_cost = np.random.binomial(1, prob_complications_untreated, (untreated_1, trials)) * cost_of_treatment_1\n",
    "    untreated_2_cost = np.random.binomial(1, prob_complications_untreated, (untreated_2, trials)) * cost_of_treatment_2\n",
    "    \n",
    "    false_neg_1_cost = false_neg_1_cost.sum(axis=0)\n",
    "    expected_false_neg_1_cost = false_neg_1_cost.mean()\n",
    "    \n",
    "    false_neg_2_cost = false_neg_2_cost.sum(axis=0)\n",
    "    expected_false_neg_2_cost = false_neg_2_cost.mean()\n",
    "    \n",
    "    untreated_1_cost = untreated_1_cost.sum(axis=0)\n",
    "    expected_untreated_1_cost = untreated_1_cost.mean()\n",
    "    \n",
    "    untreated_2_cost = untreated_2_cost.sum(axis=0)\n",
    "    expected_untreated_2_cost = untreated_2_cost.mean()\n",
    "    \n",
    "    total_cost = vaccine_cost + intervention_cost + expected_false_neg_1_cost + expected_false_neg_2_cost + expected_untreated_1_cost + expected_untreated_2_cost\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're just going to take the weighted logistic model, again, and tune the regularization parameter to both beat the benchmark on the observed data and minimize expected cost on unseen data (i.e. to prevent ***overfitting***). Instead of using 'balanced' class weights, we're using a custom weighting scheme for the three classes (this parameter should really be tuned!).\n",
    "\n",
    "It would probally also be go through the whole \"choosing a model, tuning these models\"-process again, this time to minimize cost.\n",
    "\n",
    "**Note:** Be aware that the cost is now sensitive to sample size! The smaller the pool of patients the less the cost. If you are evaluating cost on a held-out test set then you can artificially make the cost very small. The benchmarks we give are for the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = df.values[:, :-2]\n",
    "y = df.values[:, -1]\n",
    "y = y - 1\n",
    "\n",
    "x_test = df_test.values[:, :-2]\n",
    "y_test = df_test.values[:, -1]\n",
    "\n",
    "y_test = y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = lambda model, x_test, y_test: pd.Series([model.score(x_test, y_test), \n",
    "                                                 model.score(x_test[y_test==0], y_test[y_test==0]),\n",
    "                                                 model.score(x_test[y_test==1], y_test[y_test==1]), \n",
    "                                                 model.score(x_test[y_test==2], y_test[y_test==2]), \n",
    "                                                 cost(y_test, model.predict(x_test))],\n",
    "                                                index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1', 'accuracy on class 2', 'total cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.001\n",
      "C: 0.01\n",
      "C: 0.1\n",
      "C: 1.0\n",
      "C: 10.0\n",
      "C: 100.0\n",
      "C: 1000.0\n"
     ]
    }
   ],
   "source": [
    "Cs = 10.**np.arange(-3, 4, 1)\n",
    "scores = []\n",
    "for C in Cs:\n",
    "    print('C:', C)\n",
    "    weighted_log_scores = np.array([0., 0., 0., 0., 0.])\n",
    "    kf = KFold(len(x), n_folds=10, shuffle=True, random_state=10)\n",
    "    for train_index, test_index in kf:\n",
    "        x_validate_train, x_validate_test = x[train_index], x[test_index]\n",
    "        y_validate_train, y_validate_test = y[train_index], y[test_index]\n",
    "\n",
    "        weighted_logistic = LogisticRegression(C=C, class_weight={0:0.7, 1:10, 2:10})\n",
    "        weighted_logistic.fit(x_validate_train, y_validate_train)\n",
    "\n",
    "        weighted_log_scores += score(weighted_logistic, x_validate_test, y_validate_test).values\n",
    "\n",
    "    scores.append(weighted_log_scores / 10.)\n",
    "\n",
    "scores = pd.DataFrame(np.array(scores).T, columns=[str(C) for C in Cs], index=['overall accuracy', 'accuracy on class 0', 'accuracy on class 1', 'accuracy on class 2', 'total cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.001</th>\n",
       "      <th>0.01</th>\n",
       "      <th>0.1</th>\n",
       "      <th>1.0</th>\n",
       "      <th>10.0</th>\n",
       "      <th>100.0</th>\n",
       "      <th>1000.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall accuracy</th>\n",
       "      <td>0.930048</td>\n",
       "      <td>0.928904</td>\n",
       "      <td>0.929286</td>\n",
       "      <td>0.928333</td>\n",
       "      <td>0.927951</td>\n",
       "      <td>0.929095</td>\n",
       "      <td>0.929475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 0</th>\n",
       "      <td>0.984187</td>\n",
       "      <td>0.982758</td>\n",
       "      <td>0.982768</td>\n",
       "      <td>0.981745</td>\n",
       "      <td>0.981749</td>\n",
       "      <td>0.982970</td>\n",
       "      <td>0.982972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 1</th>\n",
       "      <td>0.069623</td>\n",
       "      <td>0.062262</td>\n",
       "      <td>0.064524</td>\n",
       "      <td>0.075595</td>\n",
       "      <td>0.065119</td>\n",
       "      <td>0.060119</td>\n",
       "      <td>0.073132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy on class 2</th>\n",
       "      <td>0.074134</td>\n",
       "      <td>0.130087</td>\n",
       "      <td>0.133983</td>\n",
       "      <td>0.108225</td>\n",
       "      <td>0.099892</td>\n",
       "      <td>0.114177</td>\n",
       "      <td>0.099892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total cost</th>\n",
       "      <td>657048.050000</td>\n",
       "      <td>658235.900000</td>\n",
       "      <td>654322.300000</td>\n",
       "      <td>657130.500000</td>\n",
       "      <td>661399.300000</td>\n",
       "      <td>656825.250000</td>\n",
       "      <td>653456.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0.001           0.01            0.1  \\\n",
       "overall accuracy          0.930048       0.928904       0.929286   \n",
       "accuracy on class 0       0.984187       0.982758       0.982768   \n",
       "accuracy on class 1       0.069623       0.062262       0.064524   \n",
       "accuracy on class 2       0.074134       0.130087       0.133983   \n",
       "total cost           657048.050000  658235.900000  654322.300000   \n",
       "\n",
       "                               1.0           10.0          100.0  \\\n",
       "overall accuracy          0.928333       0.927951       0.929095   \n",
       "accuracy on class 0       0.981745       0.981749       0.982970   \n",
       "accuracy on class 1       0.075595       0.065119       0.060119   \n",
       "accuracy on class 2       0.108225       0.099892       0.114177   \n",
       "total cost           657130.500000  661399.300000  656825.250000   \n",
       "\n",
       "                            1000.0  \n",
       "overall accuracy          0.929475  \n",
       "accuracy on class 0       0.982972  \n",
       "accuracy on class 1       0.073132  \n",
       "accuracy on class 2       0.099892  \n",
       "total cost           653456.600000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall accuracy       9.321388e-01\n",
       "accuracy on class 0    9.829822e-01\n",
       "accuracy on class 1    1.013216e-01\n",
       "accuracy on class 2    1.807229e-01\n",
       "total cost             6.256906e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Weighted logistic regression\n",
    "weighted_logistic = LogisticRegression(C=100, class_weight={0:0.7, 1:10, 2:10})\n",
    "weighted_logistic.fit(x, y)\n",
    "weighted_log_scores = score(weighted_logistic, x, y)\n",
    "weighted_log_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall accuracy       9.321388e-01\n",
       "accuracy on class 0    9.829822e-01\n",
       "accuracy on class 1    1.013216e-01\n",
       "accuracy on class 2    1.807229e-01\n",
       "total cost             6.259606e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Weighted logistic regression\n",
    "weighted_log_scores = score(weighted_logistic, x_test, y_test)\n",
    "weighted_log_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimimum cost on train: 1368840.0\n",
      "minimimum cost on test: 1368840.0\n"
     ]
    }
   ],
   "source": [
    "print('minimimum cost on train:', cost(y, y))\n",
    "print('minimimum cost on test:', cost(y_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple model cost on train: 6839171.0\n",
      "simple model cost on test: 6854941.0\n"
     ]
    }
   ],
   "source": [
    "print('simple model cost on train:', cost(y, np.array([0] * len(y))))\n",
    "print('simple model cost on test:', cost(y_test, np.array([0] * len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple model cost on train: 22332410.0\n",
      "simple model cost on test: 22334345.0\n"
     ]
    }
   ],
   "source": [
    "print('simple model cost on train:', cost(y, np.array([1] * len(y))))\n",
    "print('simple model cost on test:', cost(y_test, np.array([1] * len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple model cost on train: 24305541.5\n",
      "simple model cost on test: 24298314.0\n"
     ]
    }
   ],
   "source": [
    "print('simple model cost on train:', cost(y, np.array([2] * len(y))))\n",
    "print('simple model cost on test:', cost(y_test, np.array([2] * len(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We beat the benchmarks on the observed data and did pretty well on test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = lambda y_true, y_pred: (((y_true - y_pred) == 0).sum() * 1.) / len(y_true)\n",
    "\n",
    "# function to check - did they beat our benchmarks?\n",
    "# You can either pass it the flu_predict function the student wrote or the\n",
    "# file name of the y-labels.\n",
    "#\n",
    "# predict: function of type (array -> array)\n",
    "# pred_y_file_name: file name of where their predicted y-labels live\n",
    "# data_preprocessing: if false x-train data will be fed to flu_predict with no processing if true data will be encoded with 0 filled in for NaN\n",
    "# cost: if true computes the expect cost\n",
    "# \n",
    "# return some string indicating result of comparison with benchmark\n",
    "\n",
    "def beat_benchmark(flu_predict=None, pred_y_file_name=None, data_preprocessing=False):\n",
    "    \n",
    "    acc_bm_0 = .69\n",
    "    acc_bm_1 = .60\n",
    "    \n",
    "    acc_rm_0 = .5\n",
    "    acc_rm_1 = .5\n",
    "    \n",
    "    df_test = pd.read_csv('https://raw.githubusercontent.com/cs109/a-2017/master/Midterms/2016%20Midterm%202/data/flu_test.csv')\n",
    "    df_test = df_test[~np.isnan(df_test['flu'])]\n",
    "    df_test['flutype'] = df_test['flutype'] - 1\n",
    "    \n",
    "    y_true = df_test.values[:, -2]\n",
    "    \n",
    "    if flu_predict is not None:\n",
    "        if data_preprocessing:                       \n",
    "            encode = preprocessing.LabelEncoder()\n",
    "            for column in df_test.columns:\n",
    "                if df_test[column].dtype == np.object:\n",
    "                    df_test.loc[:, column] = encode.fit_transform(df_test[column])\n",
    "\n",
    "            df_test = df_test.fillna(0)\n",
    "            \n",
    "        x = df_test.values[:, :-2]\n",
    "        \n",
    "        y_pred = flu_predict(x)\n",
    "        \n",
    "    elif pred_y_file_name is not None:\n",
    "        df_y_pred = pd.read_csv(pred_y_file_name)\n",
    "        y_pred = df_y_pred.values[:, -1]\n",
    "        \n",
    "    else:\n",
    "        return 'params ill-specified'\n",
    "    \n",
    "    acc_0 = accuracy(y_true[y_true == 0], y_pred[y_true == 0])\n",
    "    acc_1 = accuracy(y_true[y_true == 1], y_pred[y_true == 1])\n",
    "    \n",
    "    if acc_0 > acc_rm_0 and acc_1 > acc_rm_1:\n",
    "        print ('accuracies: {}, {}'.format(acc_0, acc_1))\n",
    "        return \"accuracy: beats all benchmarks :)\"\n",
    "    elif acc_0 > acc_bm_0 and acc_1 > acc_bm_1:\n",
    "        print ('accuracies: {}, {}'.format(acc_0, acc_1))\n",
    "        return \"accuracy: beats only baseline models :/\"\n",
    "    else:\n",
    "        print ('accuracies: {}, {}'.format(acc_0, acc_1))\n",
    "        return \"accuracy: beats no benchmarks :(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies: 1.0, 0.0\n",
      "accuracy: beats no benchmarks :(\n"
     ]
    }
   ],
   "source": [
    "# Example 0: using the autograding function\n",
    "\n",
    "def flu_predict(x):\n",
    "    return np.array([0] * len(x))\n",
    "\n",
    "print(beat_benchmark(flu_predict, data_preprocessing=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies: 0.7191558441558441, 0.6112956810631229\n",
      "accuracy: beats all benchmarks :)\n"
     ]
    }
   ],
   "source": [
    "# Example 1: using the autograding function\n",
    "print(beat_benchmark(pred_y_file_name='https://raw.githubusercontent.com/cs109/a-2017/master/Midterms/2016%20Midterm%202/data/Example_1.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
